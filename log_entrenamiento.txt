Files already downloaded and verified
Files already downloaded and verified
Dataset de entrenamiento: 50000 imágenes
Dataset de validación: 10000 imágenes
Número de batches de entrenamiento: 1563
Número de batches de validación: 313
Modelo creado y movido a: cuda
Número de parámetros: 20,986,938
Iniciando entrenamiento...
==================================================
Batch 0/1563, Loss: 2.5265, Acc: 9.38%
Batch 100/1563, Loss: 1.8431, Acc: 21.63%
Batch 200/1563, Loss: 1.8429, Acc: 23.07%
Batch 300/1563, Loss: 1.6857, Acc: 25.71%
Batch 400/1563, Loss: 1.7768, Acc: 27.40%
Batch 500/1563, Loss: 1.6242, Acc: 29.26%
Batch 600/1563, Loss: 1.3736, Acc: 30.71%
Batch 700/1563, Loss: 1.3864, Acc: 31.74%
Batch 800/1563, Loss: 1.5004, Acc: 32.72%
Batch 900/1563, Loss: 1.6017, Acc: 33.70%
Batch 1000/1563, Loss: 1.6122, Acc: 34.52%
Batch 1100/1563, Loss: 1.3004, Acc: 35.16%
Batch 1200/1563, Loss: 1.2281, Acc: 35.91%
Batch 1300/1563, Loss: 1.4146, Acc: 36.63%
Batch 1400/1563, Loss: 1.5466, Acc: 37.41%
Batch 1500/1563, Loss: 1.2718, Acc: 38.00%
Epoch 1/50:
  Train Loss: 1.6934, Train Acc: 38.41%
  Val Loss: 1.4608, Val Acc: 46.94%
  Time: 552.98s, LR: 0.000300
  ✓ Nuevo mejor modelo guardado! Val Acc: 46.94%
--------------------------------------------------
Batch 0/1563, Loss: 1.3379, Acc: 56.25%
Batch 100/1563, Loss: 1.3241, Acc: 48.05%
Batch 200/1563, Loss: 1.2043, Acc: 48.99%
Batch 300/1563, Loss: 1.2462, Acc: 49.20%
Batch 400/1563, Loss: 1.1990, Acc: 49.04%
Batch 500/1563, Loss: 1.1452, Acc: 49.06%
Batch 600/1563, Loss: 1.1796, Acc: 49.52%
Batch 700/1563, Loss: 1.2980, Acc: 49.63%
Batch 800/1563, Loss: 1.1484, Acc: 49.68%
Batch 900/1563, Loss: 1.2560, Acc: 49.91%
Batch 1000/1563, Loss: 1.2761, Acc: 50.27%
Batch 1100/1563, Loss: 1.4183, Acc: 50.53%
Batch 1200/1563, Loss: 1.1270, Acc: 50.76%
Batch 1300/1563, Loss: 1.2992, Acc: 51.00%
Batch 1400/1563, Loss: 1.2599, Acc: 51.09%
Batch 1500/1563, Loss: 1.2118, Acc: 51.18%
Epoch 2/50:
  Train Loss: 1.3499, Train Acc: 51.20%
  Val Loss: 1.2470, Val Acc: 53.38%
  Time: 515.61s, LR: 0.000299
  ✓ Nuevo mejor modelo guardado! Val Acc: 53.38%
--------------------------------------------------
Batch 0/1563, Loss: 1.1786, Acc: 59.38%
Batch 100/1563, Loss: 1.5457, Acc: 56.37%
Batch 200/1563, Loss: 1.3775, Acc: 55.60%
Batch 300/1563, Loss: 1.0034, Acc: 55.24%
Batch 400/1563, Loss: 1.4726, Acc: 55.46%
Batch 500/1563, Loss: 1.3784, Acc: 55.32%
Batch 600/1563, Loss: 1.2744, Acc: 55.32%
Batch 700/1563, Loss: 1.6352, Acc: 55.39%
Batch 800/1563, Loss: 1.2334, Acc: 55.29%
Batch 900/1563, Loss: 1.2221, Acc: 55.34%
Batch 1000/1563, Loss: 0.9639, Acc: 55.47%
Batch 1100/1563, Loss: 0.9086, Acc: 55.64%
Batch 1200/1563, Loss: 1.2252, Acc: 55.75%
Batch 1300/1563, Loss: 0.9472, Acc: 55.83%
Batch 1400/1563, Loss: 1.3185, Acc: 56.02%
Batch 1500/1563, Loss: 1.0325, Acc: 56.06%
Epoch 3/50:
  Train Loss: 1.2196, Train Acc: 56.14%
  Val Loss: 1.1966, Val Acc: 56.57%
  Time: 501.77s, LR: 0.000297
  ✓ Nuevo mejor modelo guardado! Val Acc: 56.57%
--------------------------------------------------
Batch 0/1563, Loss: 1.2686, Acc: 43.75%
Batch 100/1563, Loss: 1.2687, Acc: 59.53%
Batch 200/1563, Loss: 1.2727, Acc: 58.78%
Batch 300/1563, Loss: 1.3613, Acc: 58.64%
Batch 400/1563, Loss: 1.2023, Acc: 58.57%
Batch 500/1563, Loss: 0.7812, Acc: 58.84%
Batch 600/1563, Loss: 1.1035, Acc: 58.93%
Batch 700/1563, Loss: 1.2618, Acc: 58.96%
Batch 800/1563, Loss: 1.4038, Acc: 59.03%
Batch 900/1563, Loss: 1.2097, Acc: 59.01%
Batch 1000/1563, Loss: 1.1096, Acc: 59.01%
Batch 1100/1563, Loss: 0.9183, Acc: 59.03%
Batch 1200/1563, Loss: 1.0273, Acc: 59.11%
Batch 1300/1563, Loss: 0.6570, Acc: 59.16%
Batch 1400/1563, Loss: 1.1488, Acc: 59.19%
Batch 1500/1563, Loss: 1.2129, Acc: 59.29%
Epoch 4/50:
  Train Loss: 1.1334, Train Acc: 59.34%
  Val Loss: 1.0999, Val Acc: 60.84%
  Time: 503.69s, LR: 0.000295
  ✓ Nuevo mejor modelo guardado! Val Acc: 60.84%
--------------------------------------------------
Batch 0/1563, Loss: 1.1240, Acc: 56.25%
Batch 100/1563, Loss: 1.1584, Acc: 61.42%
Batch 200/1563, Loss: 1.2962, Acc: 60.84%
Batch 300/1563, Loss: 1.3275, Acc: 61.49%
Batch 400/1563, Loss: 1.2862, Acc: 61.57%
Batch 500/1563, Loss: 1.0421, Acc: 61.51%
Batch 600/1563, Loss: 0.7490, Acc: 61.50%
Batch 700/1563, Loss: 1.3938, Acc: 61.59%
Batch 800/1563, Loss: 0.9914, Acc: 61.47%
Batch 900/1563, Loss: 0.9121, Acc: 61.46%
Batch 1000/1563, Loss: 0.9302, Acc: 61.51%
Batch 1100/1563, Loss: 1.1316, Acc: 61.57%
Batch 1200/1563, Loss: 1.0812, Acc: 61.59%
Batch 1300/1563, Loss: 1.1725, Acc: 61.72%
Batch 1400/1563, Loss: 1.3903, Acc: 61.68%
Batch 1500/1563, Loss: 1.2295, Acc: 61.70%
Epoch 5/50:
  Train Loss: 1.0673, Train Acc: 61.71%
  Val Loss: 1.0308, Val Acc: 62.98%
  Time: 504.18s, LR: 0.000293
  ✓ Nuevo mejor modelo guardado! Val Acc: 62.98%
--------------------------------------------------
Batch 0/1563, Loss: 0.9210, Acc: 62.50%
Batch 100/1563, Loss: 0.9093, Acc: 65.01%
Batch 200/1563, Loss: 0.7634, Acc: 64.52%
Batch 300/1563, Loss: 1.1196, Acc: 64.05%
Batch 400/1563, Loss: 1.0249, Acc: 64.19%
Batch 500/1563, Loss: 1.3919, Acc: 63.98%
Batch 600/1563, Loss: 1.2182, Acc: 63.81%
Batch 700/1563, Loss: 0.8093, Acc: 63.92%
Batch 800/1563, Loss: 1.1931, Acc: 63.81%
Batch 900/1563, Loss: 1.3049, Acc: 63.81%
Batch 1000/1563, Loss: 1.0859, Acc: 63.72%
Batch 1100/1563, Loss: 0.8387, Acc: 63.72%
Batch 1200/1563, Loss: 1.1184, Acc: 63.65%
Batch 1300/1563, Loss: 0.9787, Acc: 63.69%
Batch 1400/1563, Loss: 1.0261, Acc: 63.73%
Batch 1500/1563, Loss: 0.9017, Acc: 63.79%
Epoch 6/50:
  Train Loss: 1.0101, Train Acc: 63.84%
  Val Loss: 1.0042, Val Acc: 64.29%
  Time: 503.99s, LR: 0.000289
  ✓ Nuevo mejor modelo guardado! Val Acc: 64.29%
--------------------------------------------------
Batch 0/1563, Loss: 0.5148, Acc: 78.12%
Batch 100/1563, Loss: 0.6401, Acc: 65.50%
Batch 200/1563, Loss: 0.7559, Acc: 65.59%
Batch 300/1563, Loss: 0.7344, Acc: 65.28%
Batch 400/1563, Loss: 0.7546, Acc: 65.54%
Batch 500/1563, Loss: 1.0383, Acc: 65.75%
Batch 600/1563, Loss: 1.0511, Acc: 65.90%
Batch 700/1563, Loss: 1.0223, Acc: 65.71%
Batch 800/1563, Loss: 1.0950, Acc: 65.78%
Batch 900/1563, Loss: 0.7569, Acc: 65.71%
Batch 1000/1563, Loss: 0.9178, Acc: 65.70%
Batch 1100/1563, Loss: 0.9620, Acc: 65.70%
Batch 1200/1563, Loss: 0.9371, Acc: 65.69%
Batch 1300/1563, Loss: 1.0356, Acc: 65.77%
Batch 1400/1563, Loss: 1.2471, Acc: 65.81%
Batch 1500/1563, Loss: 0.8556, Acc: 65.76%
Epoch 7/50:
  Train Loss: 0.9613, Train Acc: 65.80%
  Val Loss: 0.9724, Val Acc: 64.54%
  Time: 505.08s, LR: 0.000286
  ✓ Nuevo mejor modelo guardado! Val Acc: 64.54%
--------------------------------------------------
Batch 0/1563, Loss: 0.9505, Acc: 75.00%
Batch 100/1563, Loss: 0.6712, Acc: 67.17%
Batch 200/1563, Loss: 0.8913, Acc: 67.24%
Batch 300/1563, Loss: 1.1015, Acc: 67.11%
Batch 400/1563, Loss: 0.8167, Acc: 67.34%
Batch 500/1563, Loss: 1.1390, Acc: 67.05%
Batch 600/1563, Loss: 1.0132, Acc: 67.06%
Batch 700/1563, Loss: 1.1421, Acc: 67.17%
Batch 800/1563, Loss: 1.1129, Acc: 67.13%
Batch 900/1563, Loss: 0.6836, Acc: 67.11%
Batch 1000/1563, Loss: 0.8564, Acc: 67.20%
Batch 1100/1563, Loss: 0.9462, Acc: 67.23%
Batch 1200/1563, Loss: 0.7390, Acc: 67.29%
Batch 1300/1563, Loss: 1.1890, Acc: 67.31%
Batch 1400/1563, Loss: 0.9434, Acc: 67.21%
Batch 1500/1563, Loss: 0.8401, Acc: 67.28%
Epoch 8/50:
  Train Loss: 0.9155, Train Acc: 67.32%
  Val Loss: 0.9381, Val Acc: 66.47%
  Time: 503.20s, LR: 0.000281
  ✓ Nuevo mejor modelo guardado! Val Acc: 66.47%
--------------------------------------------------
Batch 0/1563, Loss: 0.9765, Acc: 71.88%
Batch 100/1563, Loss: 0.9723, Acc: 67.30%
Batch 200/1563, Loss: 0.9497, Acc: 68.25%
Batch 300/1563, Loss: 0.9459, Acc: 68.17%
Batch 400/1563, Loss: 0.7725, Acc: 68.09%
Batch 500/1563, Loss: 0.8510, Acc: 68.27%
Batch 600/1563, Loss: 0.9834, Acc: 68.48%
Batch 700/1563, Loss: 0.8944, Acc: 68.31%
Batch 800/1563, Loss: 0.5829, Acc: 68.43%
Batch 900/1563, Loss: 1.2001, Acc: 68.55%
Batch 1000/1563, Loss: 0.9206, Acc: 68.59%
Batch 1100/1563, Loss: 0.9637, Acc: 68.69%
Batch 1200/1563, Loss: 0.9883, Acc: 68.66%
Batch 1300/1563, Loss: 0.6967, Acc: 68.70%
Batch 1400/1563, Loss: 1.0110, Acc: 68.60%
Batch 1500/1563, Loss: 1.2178, Acc: 68.68%
Epoch 9/50:
  Train Loss: 0.8776, Train Acc: 68.70%
  Val Loss: 0.9180, Val Acc: 67.73%
  Time: 503.90s, LR: 0.000277
  ✓ Nuevo mejor modelo guardado! Val Acc: 67.73%
--------------------------------------------------
Batch 0/1563, Loss: 0.6123, Acc: 75.00%
Batch 100/1563, Loss: 0.6641, Acc: 71.01%
Batch 200/1563, Loss: 0.7837, Acc: 71.13%
Batch 300/1563, Loss: 0.9543, Acc: 70.95%
Batch 400/1563, Loss: 1.0878, Acc: 71.10%
Batch 500/1563, Loss: 1.1916, Acc: 70.66%
Batch 600/1563, Loss: 0.9983, Acc: 70.47%
Batch 700/1563, Loss: 1.0432, Acc: 70.44%
Batch 800/1563, Loss: 0.6397, Acc: 70.36%
Batch 900/1563, Loss: 0.6295, Acc: 70.29%
Batch 1000/1563, Loss: 0.7751, Acc: 70.50%
Batch 1100/1563, Loss: 1.1411, Acc: 70.58%
Batch 1200/1563, Loss: 0.6155, Acc: 70.59%
Batch 1300/1563, Loss: 0.7887, Acc: 70.59%
Batch 1400/1563, Loss: 0.5552, Acc: 70.50%
Batch 1500/1563, Loss: 0.9811, Acc: 70.38%
Epoch 10/50:
  Train Loss: 0.8332, Train Acc: 70.40%
  Val Loss: 0.9072, Val Acc: 67.95%
  Time: 507.28s, LR: 0.000271
  ✓ Nuevo mejor modelo guardado! Val Acc: 67.95%
--------------------------------------------------
Batch 0/1563, Loss: 0.5877, Acc: 78.12%
Batch 100/1563, Loss: 0.8341, Acc: 71.60%
Batch 200/1563, Loss: 0.9070, Acc: 72.19%
Batch 300/1563, Loss: 0.8930, Acc: 71.98%
Batch 400/1563, Loss: 0.5988, Acc: 72.10%
Batch 500/1563, Loss: 0.7313, Acc: 71.96%
Batch 600/1563, Loss: 1.2384, Acc: 71.93%
Batch 700/1563, Loss: 0.9500, Acc: 71.76%
Batch 800/1563, Loss: 0.5027, Acc: 71.67%
Batch 900/1563, Loss: 0.6168, Acc: 71.61%
Batch 1000/1563, Loss: 0.6315, Acc: 71.48%
Batch 1100/1563, Loss: 0.8545, Acc: 71.46%
Batch 1200/1563, Loss: 0.8165, Acc: 71.35%
Batch 1300/1563, Loss: 0.9918, Acc: 71.26%
Batch 1400/1563, Loss: 1.0202, Acc: 71.26%
Batch 1500/1563, Loss: 0.7561, Acc: 71.20%
Epoch 11/50:
  Train Loss: 0.8048, Train Acc: 71.07%
  Val Loss: 0.8342, Val Acc: 70.54%
  Time: 505.34s, LR: 0.000266
  ✓ Nuevo mejor modelo guardado! Val Acc: 70.54%
--------------------------------------------------
Batch 0/1563, Loss: 0.7545, Acc: 75.00%
Batch 100/1563, Loss: 0.8755, Acc: 74.47%
Batch 200/1563, Loss: 0.6179, Acc: 73.24%
Batch 300/1563, Loss: 0.8003, Acc: 73.41%
Batch 400/1563, Loss: 0.6483, Acc: 73.19%
Batch 500/1563, Loss: 0.7573, Acc: 73.22%
Batch 600/1563, Loss: 1.0045, Acc: 73.06%
Batch 700/1563, Loss: 0.9260, Acc: 72.96%
Batch 800/1563, Loss: 1.3723, Acc: 73.01%
Batch 900/1563, Loss: 0.9368, Acc: 73.12%
Batch 1000/1563, Loss: 0.5821, Acc: 72.96%
Batch 1100/1563, Loss: 0.6984, Acc: 72.83%
Batch 1200/1563, Loss: 0.9081, Acc: 72.79%
Batch 1300/1563, Loss: 0.7933, Acc: 72.82%
Batch 1400/1563, Loss: 0.8917, Acc: 72.78%
Batch 1500/1563, Loss: 0.7475, Acc: 72.70%
Epoch 12/50:
  Train Loss: 0.7639, Train Acc: 72.67%
  Val Loss: 0.8501, Val Acc: 69.80%
  Time: 504.72s, LR: 0.000259
--------------------------------------------------
Batch 0/1563, Loss: 0.5724, Acc: 78.12%
Batch 100/1563, Loss: 0.8566, Acc: 74.78%
Batch 200/1563, Loss: 0.6321, Acc: 75.23%
Batch 300/1563, Loss: 0.8709, Acc: 74.80%
Batch 400/1563, Loss: 0.4447, Acc: 74.54%
Batch 500/1563, Loss: 0.6595, Acc: 74.15%
Batch 600/1563, Loss: 0.7500, Acc: 74.16%
Batch 700/1563, Loss: 0.7875, Acc: 74.08%
Batch 800/1563, Loss: 0.8738, Acc: 74.04%
Batch 900/1563, Loss: 1.2652, Acc: 73.95%
Batch 1000/1563, Loss: 0.7671, Acc: 73.97%
Batch 1100/1563, Loss: 0.8120, Acc: 73.99%
Batch 1200/1563, Loss: 0.7017, Acc: 73.94%
Batch 1300/1563, Loss: 1.0393, Acc: 73.97%
Batch 1400/1563, Loss: 0.9470, Acc: 74.14%
Batch 1500/1563, Loss: 0.8866, Acc: 74.08%
Epoch 13/50:
  Train Loss: 0.7283, Train Acc: 74.03%
  Val Loss: 0.8082, Val Acc: 71.54%
  Time: 511.72s, LR: 0.000253
  ✓ Nuevo mejor modelo guardado! Val Acc: 71.54%
--------------------------------------------------
Batch 0/1563, Loss: 0.6277, Acc: 78.12%
Batch 100/1563, Loss: 0.6413, Acc: 77.20%
Batch 200/1563, Loss: 0.7302, Acc: 76.49%
Batch 300/1563, Loss: 0.7204, Acc: 75.81%
Batch 400/1563, Loss: 0.5141, Acc: 75.69%
Batch 500/1563, Loss: 0.6308, Acc: 75.75%
Batch 600/1563, Loss: 0.7150, Acc: 75.54%
Batch 700/1563, Loss: 0.6351, Acc: 75.45%
Batch 800/1563, Loss: 0.8224, Acc: 75.25%
Batch 900/1563, Loss: 0.7660, Acc: 75.18%
Batch 1000/1563, Loss: 0.7813, Acc: 75.11%
Batch 1100/1563, Loss: 0.7011, Acc: 75.07%
Batch 1200/1563, Loss: 0.5858, Acc: 75.08%
Batch 1300/1563, Loss: 0.8410, Acc: 75.09%
Batch 1400/1563, Loss: 0.6823, Acc: 75.15%
Batch 1500/1563, Loss: 0.7872, Acc: 75.24%
Epoch 14/50:
  Train Loss: 0.6987, Train Acc: 75.20%
  Val Loss: 0.8060, Val Acc: 71.74%
  Time: 510.15s, LR: 0.000246
  ✓ Nuevo mejor modelo guardado! Val Acc: 71.74%
--------------------------------------------------
Batch 0/1563, Loss: 0.6435, Acc: 81.25%
Batch 100/1563, Loss: 0.5688, Acc: 77.29%
Batch 200/1563, Loss: 0.3637, Acc: 77.69%
Batch 300/1563, Loss: 0.3963, Acc: 77.30%
Batch 400/1563, Loss: 0.4098, Acc: 77.18%
Batch 500/1563, Loss: 0.6018, Acc: 77.06%
Batch 600/1563, Loss: 0.3510, Acc: 76.93%
Batch 700/1563, Loss: 0.5092, Acc: 76.85%
Batch 800/1563, Loss: 0.6461, Acc: 76.70%
Batch 900/1563, Loss: 0.3234, Acc: 76.63%
Batch 1000/1563, Loss: 0.5376, Acc: 76.71%
Batch 1100/1563, Loss: 0.5223, Acc: 76.71%
Batch 1200/1563, Loss: 0.5436, Acc: 76.64%
Batch 1300/1563, Loss: 0.6091, Acc: 76.56%
Batch 1400/1563, Loss: 0.4547, Acc: 76.47%
Batch 1500/1563, Loss: 0.7159, Acc: 76.46%
Epoch 15/50:
  Train Loss: 0.6606, Train Acc: 76.44%
  Val Loss: 0.7942, Val Acc: 72.61%
  Time: 507.20s, LR: 0.000238
  ✓ Nuevo mejor modelo guardado! Val Acc: 72.61%
--------------------------------------------------
Batch 0/1563, Loss: 0.2545, Acc: 84.38%
Batch 100/1563, Loss: 0.7353, Acc: 78.12%
Batch 200/1563, Loss: 0.4850, Acc: 78.64%
Batch 300/1563, Loss: 0.5712, Acc: 78.58%
Batch 400/1563, Loss: 0.5604, Acc: 78.16%
Batch 500/1563, Loss: 0.8400, Acc: 78.12%
Batch 600/1563, Loss: 0.7262, Acc: 77.88%
Batch 700/1563, Loss: 0.6055, Acc: 77.88%
Batch 800/1563, Loss: 0.4697, Acc: 77.86%
Batch 900/1563, Loss: 0.4511, Acc: 77.81%
Batch 1000/1563, Loss: 0.7756, Acc: 77.73%
Batch 1100/1563, Loss: 0.5770, Acc: 77.70%
Batch 1200/1563, Loss: 0.9480, Acc: 77.60%
Batch 1300/1563, Loss: 0.5243, Acc: 77.56%
Batch 1400/1563, Loss: 0.4697, Acc: 77.54%
Batch 1500/1563, Loss: 0.7961, Acc: 77.44%
Epoch 16/50:
  Train Loss: 0.6311, Train Acc: 77.45%
  Val Loss: 0.8119, Val Acc: 71.85%
  Time: 506.93s, LR: 0.000230
--------------------------------------------------
Batch 0/1563, Loss: 0.8125, Acc: 65.62%
Batch 100/1563, Loss: 0.5264, Acc: 80.20%
Batch 200/1563, Loss: 0.7258, Acc: 79.52%
Batch 300/1563, Loss: 0.8139, Acc: 79.43%
Batch 400/1563, Loss: 0.5155, Acc: 79.34%
Batch 500/1563, Loss: 0.4062, Acc: 79.42%
Batch 600/1563, Loss: 0.8823, Acc: 79.08%
Batch 700/1563, Loss: 0.8593, Acc: 79.09%
Batch 800/1563, Loss: 0.4466, Acc: 79.17%
Batch 900/1563, Loss: 0.4821, Acc: 79.18%
Batch 1000/1563, Loss: 0.6910, Acc: 79.16%
Batch 1100/1563, Loss: 0.5948, Acc: 79.06%
Batch 1200/1563, Loss: 0.3567, Acc: 78.93%
Batch 1300/1563, Loss: 0.7013, Acc: 78.78%
Batch 1400/1563, Loss: 0.4252, Acc: 78.74%
Batch 1500/1563, Loss: 0.8506, Acc: 78.81%
Epoch 17/50:
  Train Loss: 0.6008, Train Acc: 78.70%
  Val Loss: 0.7612, Val Acc: 73.77%
  Time: 508.01s, LR: 0.000222
  ✓ Nuevo mejor modelo guardado! Val Acc: 73.77%
--------------------------------------------------
Batch 0/1563, Loss: 0.4714, Acc: 90.62%
Batch 100/1563, Loss: 0.5910, Acc: 81.56%
Batch 200/1563, Loss: 0.2176, Acc: 81.27%
Batch 300/1563, Loss: 0.6086, Acc: 80.85%
Batch 400/1563, Loss: 0.4426, Acc: 80.62%
Batch 500/1563, Loss: 0.4477, Acc: 80.29%
Batch 600/1563, Loss: 0.6082, Acc: 80.24%
Batch 700/1563, Loss: 0.3809, Acc: 80.11%
Batch 800/1563, Loss: 0.5125, Acc: 79.92%
Batch 900/1563, Loss: 0.7208, Acc: 79.87%
Batch 1000/1563, Loss: 0.6478, Acc: 79.87%
Batch 1100/1563, Loss: 0.5624, Acc: 79.86%
Batch 1200/1563, Loss: 0.5204, Acc: 79.77%
Batch 1300/1563, Loss: 0.2687, Acc: 79.76%
Batch 1400/1563, Loss: 0.9632, Acc: 79.71%
Batch 1500/1563, Loss: 0.2114, Acc: 79.62%
Epoch 18/50:
  Train Loss: 0.5694, Train Acc: 79.61%
  Val Loss: 0.7561, Val Acc: 74.36%
  Time: 509.07s, LR: 0.000214
  ✓ Nuevo mejor modelo guardado! Val Acc: 74.36%
--------------------------------------------------
Batch 0/1563, Loss: 0.9397, Acc: 68.75%
Batch 100/1563, Loss: 0.6708, Acc: 82.83%
Batch 200/1563, Loss: 0.4008, Acc: 82.21%
Batch 300/1563, Loss: 0.5116, Acc: 81.63%
Batch 400/1563, Loss: 0.4990, Acc: 81.30%
Batch 500/1563, Loss: 0.5380, Acc: 81.27%
Batch 600/1563, Loss: 0.3884, Acc: 81.08%
Batch 700/1563, Loss: 0.4733, Acc: 81.09%
Batch 800/1563, Loss: 0.7717, Acc: 81.05%
Batch 900/1563, Loss: 0.4470, Acc: 81.02%
Batch 1000/1563, Loss: 0.5576, Acc: 80.96%
Batch 1100/1563, Loss: 0.5162, Acc: 80.96%
Batch 1200/1563, Loss: 0.7773, Acc: 80.88%
Batch 1300/1563, Loss: 0.5729, Acc: 80.96%
Batch 1400/1563, Loss: 0.5495, Acc: 80.95%
Batch 1500/1563, Loss: 0.4343, Acc: 80.85%
Epoch 19/50:
  Train Loss: 0.5337, Train Acc: 80.81%
  Val Loss: 0.7626, Val Acc: 74.07%
  Time: 509.85s, LR: 0.000205
--------------------------------------------------
Batch 0/1563, Loss: 0.6077, Acc: 81.25%
Batch 100/1563, Loss: 0.2472, Acc: 83.26%
Batch 200/1563, Loss: 0.5935, Acc: 82.98%
Batch 300/1563, Loss: 0.5810, Acc: 82.84%
Batch 400/1563, Loss: 0.5379, Acc: 82.47%
Batch 500/1563, Loss: 0.5650, Acc: 82.44%
Batch 600/1563, Loss: 0.5726, Acc: 82.37%
Batch 700/1563, Loss: 0.7676, Acc: 82.33%
Batch 800/1563, Loss: 0.4212, Acc: 82.19%
Batch 900/1563, Loss: 0.3966, Acc: 82.22%
Batch 1000/1563, Loss: 0.6092, Acc: 82.16%
Batch 1100/1563, Loss: 0.3633, Acc: 82.18%
Batch 1200/1563, Loss: 0.7287, Acc: 82.12%
Batch 1300/1563, Loss: 0.3084, Acc: 82.05%
Batch 1400/1563, Loss: 0.7358, Acc: 82.09%
Batch 1500/1563, Loss: 0.5224, Acc: 82.07%
Epoch 20/50:
  Train Loss: 0.5107, Train Acc: 82.05%
  Val Loss: 0.7553, Val Acc: 74.47%
  Time: 510.75s, LR: 0.000196
  ✓ Nuevo mejor modelo guardado! Val Acc: 74.47%
--------------------------------------------------
Batch 0/1563, Loss: 0.3034, Acc: 87.50%
Batch 100/1563, Loss: 0.5437, Acc: 83.69%
Batch 200/1563, Loss: 0.2070, Acc: 83.92%
Batch 300/1563, Loss: 0.3229, Acc: 84.24%
Batch 400/1563, Loss: 0.5418, Acc: 84.37%
Batch 500/1563, Loss: 0.3644, Acc: 84.16%
Batch 600/1563, Loss: 0.4715, Acc: 84.10%
Batch 700/1563, Loss: 0.8357, Acc: 84.00%
Batch 800/1563, Loss: 0.4642, Acc: 83.93%
Batch 900/1563, Loss: 0.5192, Acc: 83.79%
Batch 1000/1563, Loss: 0.2726, Acc: 83.74%
Batch 1100/1563, Loss: 0.2923, Acc: 83.51%
Batch 1200/1563, Loss: 0.5267, Acc: 83.39%
Batch 1300/1563, Loss: 0.7184, Acc: 83.33%
Batch 1400/1563, Loss: 0.3555, Acc: 83.26%
Batch 1500/1563, Loss: 0.2512, Acc: 83.21%
Epoch 21/50:
  Train Loss: 0.4756, Train Acc: 83.17%
  Val Loss: 0.7723, Val Acc: 74.68%
  Time: 507.22s, LR: 0.000187
  ✓ Nuevo mejor modelo guardado! Val Acc: 74.68%
--------------------------------------------------
Batch 0/1563, Loss: 0.4670, Acc: 84.38%
Batch 100/1563, Loss: 0.4927, Acc: 84.00%
Batch 200/1563, Loss: 0.4985, Acc: 84.79%
Batch 300/1563, Loss: 0.2344, Acc: 85.10%
Batch 400/1563, Loss: 0.7596, Acc: 85.07%
Batch 500/1563, Loss: 0.3687, Acc: 84.84%
Batch 600/1563, Loss: 0.2899, Acc: 84.66%
Batch 700/1563, Loss: 0.3660, Acc: 84.55%
Batch 800/1563, Loss: 0.2606, Acc: 84.59%
Batch 900/1563, Loss: 0.9270, Acc: 84.32%
Batch 1000/1563, Loss: 0.4968, Acc: 84.22%
Batch 1100/1563, Loss: 0.3705, Acc: 84.15%
Batch 1200/1563, Loss: 0.3036, Acc: 84.09%
Batch 1300/1563, Loss: 0.3147, Acc: 84.14%
Batch 1400/1563, Loss: 0.8426, Acc: 84.12%
Batch 1500/1563, Loss: 0.2643, Acc: 84.18%
Epoch 22/50:
  Train Loss: 0.4453, Train Acc: 84.16%
  Val Loss: 0.7730, Val Acc: 74.98%
  Time: 498.95s, LR: 0.000178
  ✓ Nuevo mejor modelo guardado! Val Acc: 74.98%
--------------------------------------------------
Batch 0/1563, Loss: 0.3362, Acc: 84.38%
Batch 100/1563, Loss: 0.4811, Acc: 85.95%
Batch 200/1563, Loss: 0.3690, Acc: 85.91%
Batch 300/1563, Loss: 0.2181, Acc: 85.98%
Batch 400/1563, Loss: 0.5879, Acc: 85.86%
Batch 500/1563, Loss: 0.3942, Acc: 85.72%
Batch 600/1563, Loss: 0.5519, Acc: 85.65%
Batch 700/1563, Loss: 0.3977, Acc: 85.64%
Batch 800/1563, Loss: 0.2879, Acc: 85.42%
Batch 900/1563, Loss: 0.5789, Acc: 85.35%
Batch 1000/1563, Loss: 0.4640, Acc: 85.21%
Batch 1100/1563, Loss: 0.2884, Acc: 85.20%
Batch 1200/1563, Loss: 0.2251, Acc: 85.20%
Batch 1300/1563, Loss: 0.4759, Acc: 85.16%
Batch 1400/1563, Loss: 0.5962, Acc: 85.07%
Batch 1500/1563, Loss: 0.3848, Acc: 84.99%
Epoch 23/50:
  Train Loss: 0.4190, Train Acc: 85.01%
  Val Loss: 0.8013, Val Acc: 74.52%
  Time: 496.88s, LR: 0.000169
--------------------------------------------------
Batch 0/1563, Loss: 0.4955, Acc: 81.25%
Batch 100/1563, Loss: 0.3571, Acc: 86.48%
Batch 200/1563, Loss: 0.2078, Acc: 86.69%
Batch 300/1563, Loss: 0.4430, Acc: 86.74%
Batch 400/1563, Loss: 0.3585, Acc: 86.60%
Batch 500/1563, Loss: 0.5440, Acc: 86.51%
Batch 600/1563, Loss: 0.2578, Acc: 86.47%
Batch 700/1563, Loss: 0.3939, Acc: 86.37%
Batch 800/1563, Loss: 0.3137, Acc: 86.31%
Batch 900/1563, Loss: 0.2584, Acc: 86.34%
Batch 1000/1563, Loss: 0.3367, Acc: 86.17%
Batch 1100/1563, Loss: 0.3847, Acc: 86.06%
Batch 1200/1563, Loss: 0.5945, Acc: 86.03%
Batch 1300/1563, Loss: 0.3734, Acc: 86.03%
Batch 1400/1563, Loss: 0.2024, Acc: 86.04%
Batch 1500/1563, Loss: 0.2441, Acc: 86.05%
Epoch 24/50:
  Train Loss: 0.3919, Train Acc: 86.02%
  Val Loss: 0.7730, Val Acc: 76.02%
  Time: 497.13s, LR: 0.000159
  ✓ Nuevo mejor modelo guardado! Val Acc: 76.02%
--------------------------------------------------
Batch 0/1563, Loss: 0.2870, Acc: 84.38%
Batch 100/1563, Loss: 0.4208, Acc: 88.24%
Batch 200/1563, Loss: 0.1737, Acc: 88.31%
Batch 300/1563, Loss: 0.3246, Acc: 87.89%
Batch 400/1563, Loss: 0.2262, Acc: 87.71%
Batch 500/1563, Loss: 0.3025, Acc: 87.68%
Batch 600/1563, Loss: 0.4068, Acc: 87.60%
Batch 700/1563, Loss: 0.3211, Acc: 87.55%
Batch 800/1563, Loss: 0.5128, Acc: 87.50%
Batch 900/1563, Loss: 0.2708, Acc: 87.44%
Batch 1000/1563, Loss: 0.2529, Acc: 87.44%
Batch 1100/1563, Loss: 0.3910, Acc: 87.34%
Batch 1200/1563, Loss: 0.4230, Acc: 87.36%
Batch 1300/1563, Loss: 0.3288, Acc: 87.33%
Batch 1400/1563, Loss: 0.3044, Acc: 87.32%
Batch 1500/1563, Loss: 0.2930, Acc: 87.28%
Epoch 25/50:
  Train Loss: 0.3599, Train Acc: 87.23%
  Val Loss: 0.7823, Val Acc: 76.11%
  Time: 497.12s, LR: 0.000150
  ✓ Nuevo mejor modelo guardado! Val Acc: 76.11%
--------------------------------------------------
Batch 0/1563, Loss: 0.2691, Acc: 93.75%
Batch 100/1563, Loss: 0.5481, Acc: 88.64%
Batch 200/1563, Loss: 0.3733, Acc: 88.40%
Batch 300/1563, Loss: 0.3475, Acc: 88.41%
Batch 400/1563, Loss: 0.4876, Acc: 88.63%
Batch 500/1563, Loss: 0.3330, Acc: 88.75%
Batch 600/1563, Loss: 0.3377, Acc: 88.71%
Batch 700/1563, Loss: 0.3135, Acc: 88.60%
Batch 800/1563, Loss: 0.3549, Acc: 88.34%
Batch 900/1563, Loss: 0.4819, Acc: 88.35%
Batch 1000/1563, Loss: 0.2726, Acc: 88.25%
Batch 1100/1563, Loss: 0.1447, Acc: 88.15%
Batch 1200/1563, Loss: 0.3343, Acc: 88.13%
Batch 1300/1563, Loss: 0.3253, Acc: 88.04%
Batch 1400/1563, Loss: 0.3385, Acc: 88.08%
Batch 1500/1563, Loss: 0.2919, Acc: 87.96%
Epoch 26/50:
  Train Loss: 0.3332, Train Acc: 87.96%
  Val Loss: 0.8138, Val Acc: 76.08%
  Time: 497.72s, LR: 0.000141
--------------------------------------------------
Batch 0/1563, Loss: 0.2536, Acc: 93.75%
Batch 100/1563, Loss: 0.3216, Acc: 89.94%
Batch 200/1563, Loss: 0.1080, Acc: 90.36%
Batch 300/1563, Loss: 0.2205, Acc: 90.12%
Batch 400/1563, Loss: 0.2200, Acc: 90.02%
Batch 500/1563, Loss: 0.4960, Acc: 89.75%
Batch 600/1563, Loss: 0.2394, Acc: 89.59%
Batch 700/1563, Loss: 0.3363, Acc: 89.70%
Batch 800/1563, Loss: 0.4453, Acc: 89.58%
Batch 900/1563, Loss: 0.2277, Acc: 89.54%
Batch 1000/1563, Loss: 0.1804, Acc: 89.51%
Batch 1100/1563, Loss: 0.2321, Acc: 89.31%
Batch 1200/1563, Loss: 0.2767, Acc: 89.21%
Batch 1300/1563, Loss: 0.1030, Acc: 89.25%
Batch 1400/1563, Loss: 0.3569, Acc: 89.20%
Batch 1500/1563, Loss: 0.3361, Acc: 89.16%
Epoch 27/50:
  Train Loss: 0.3056, Train Acc: 89.19%
  Val Loss: 0.8063, Val Acc: 76.27%
  Time: 497.53s, LR: 0.000131
  ✓ Nuevo mejor modelo guardado! Val Acc: 76.27%
--------------------------------------------------
Batch 0/1563, Loss: 0.4432, Acc: 78.12%
Batch 100/1563, Loss: 0.4291, Acc: 90.10%
Batch 200/1563, Loss: 0.0686, Acc: 90.56%
Batch 300/1563, Loss: 0.1240, Acc: 90.33%
Batch 400/1563, Loss: 0.2177, Acc: 90.36%
Batch 500/1563, Loss: 0.2455, Acc: 90.40%
Batch 600/1563, Loss: 0.1482, Acc: 90.21%
Batch 700/1563, Loss: 0.4163, Acc: 90.33%
Batch 800/1563, Loss: 0.2214, Acc: 90.33%
Batch 900/1563, Loss: 0.3832, Acc: 90.31%
Batch 1000/1563, Loss: 0.4883, Acc: 90.24%
Batch 1100/1563, Loss: 0.1245, Acc: 90.23%
Batch 1200/1563, Loss: 0.2016, Acc: 90.27%
Batch 1300/1563, Loss: 0.3643, Acc: 90.12%
Batch 1400/1563, Loss: 0.4876, Acc: 90.08%
Batch 1500/1563, Loss: 0.5317, Acc: 90.04%
Epoch 28/50:
  Train Loss: 0.2794, Train Acc: 90.05%
  Val Loss: 0.8389, Val Acc: 76.83%
  Time: 497.99s, LR: 0.000122
  ✓ Nuevo mejor modelo guardado! Val Acc: 76.83%
--------------------------------------------------
Batch 0/1563, Loss: 0.2289, Acc: 90.62%
Batch 100/1563, Loss: 0.1747, Acc: 91.74%
Batch 200/1563, Loss: 0.1094, Acc: 91.45%
Batch 300/1563, Loss: 0.2373, Acc: 91.45%
Batch 400/1563, Loss: 0.2573, Acc: 91.18%
Batch 500/1563, Loss: 0.2173, Acc: 91.01%
Batch 600/1563, Loss: 0.3684, Acc: 91.04%
Batch 700/1563, Loss: 0.1924, Acc: 91.02%
Batch 800/1563, Loss: 0.1950, Acc: 91.05%
Batch 900/1563, Loss: 0.2356, Acc: 91.09%
Batch 1000/1563, Loss: 0.2859, Acc: 91.10%
Batch 1100/1563, Loss: 0.2422, Acc: 91.06%
Batch 1200/1563, Loss: 0.2449, Acc: 91.06%
Batch 1300/1563, Loss: 0.2316, Acc: 90.99%
Batch 1400/1563, Loss: 0.1554, Acc: 90.98%
Batch 1500/1563, Loss: 0.2647, Acc: 90.92%
Epoch 29/50:
  Train Loss: 0.2588, Train Acc: 90.91%
  Val Loss: 0.8337, Val Acc: 77.04%
  Time: 491.64s, LR: 0.000113
  ✓ Nuevo mejor modelo guardado! Val Acc: 77.04%
--------------------------------------------------
Batch 0/1563, Loss: 0.1447, Acc: 96.88%
Batch 100/1563, Loss: 0.4112, Acc: 91.46%
Batch 200/1563, Loss: 0.1184, Acc: 91.74%
Batch 300/1563, Loss: 0.0429, Acc: 92.10%
Batch 400/1563, Loss: 0.1657, Acc: 92.00%
Batch 500/1563, Loss: 0.0538, Acc: 92.04%
Batch 600/1563, Loss: 0.1891, Acc: 92.14%
Batch 700/1563, Loss: 0.2409, Acc: 92.07%
Batch 800/1563, Loss: 0.3114, Acc: 91.93%
Batch 900/1563, Loss: 0.3745, Acc: 91.90%
Batch 1000/1563, Loss: 0.2917, Acc: 91.81%
Batch 1100/1563, Loss: 0.1289, Acc: 91.71%
Batch 1200/1563, Loss: 0.1235, Acc: 91.68%
Batch 1300/1563, Loss: 0.1667, Acc: 91.62%
Batch 1400/1563, Loss: 0.0506, Acc: 91.59%
Batch 1500/1563, Loss: 0.1075, Acc: 91.52%
Epoch 30/50:
  Train Loss: 0.2363, Train Acc: 91.48%
  Val Loss: 0.8786, Val Acc: 76.61%
  Time: 492.20s, LR: 0.000104
--------------------------------------------------
Batch 0/1563, Loss: 0.4159, Acc: 84.38%
Batch 100/1563, Loss: 0.0964, Acc: 92.42%
Batch 200/1563, Loss: 0.2265, Acc: 93.17%
Batch 300/1563, Loss: 0.2247, Acc: 92.96%
Batch 400/1563, Loss: 0.2940, Acc: 92.99%
Batch 500/1563, Loss: 0.2006, Acc: 92.86%
Batch 600/1563, Loss: 0.1915, Acc: 92.86%
Batch 700/1563, Loss: 0.0934, Acc: 92.68%
Batch 800/1563, Loss: 0.1361, Acc: 92.59%
Batch 900/1563, Loss: 0.1050, Acc: 92.57%
Batch 1000/1563, Loss: 0.3029, Acc: 92.44%
Batch 1100/1563, Loss: 0.2156, Acc: 92.42%
Batch 1200/1563, Loss: 0.1578, Acc: 92.41%
Batch 1300/1563, Loss: 0.2918, Acc: 92.41%
Batch 1400/1563, Loss: 0.3659, Acc: 92.36%
Batch 1500/1563, Loss: 0.1109, Acc: 92.36%
Epoch 31/50:
  Train Loss: 0.2175, Train Acc: 92.35%
  Val Loss: 0.8851, Val Acc: 77.41%
  Time: 503.79s, LR: 0.000095
  ✓ Nuevo mejor modelo guardado! Val Acc: 77.41%
--------------------------------------------------
Batch 0/1563, Loss: 0.1155, Acc: 96.88%
Batch 100/1563, Loss: 0.1863, Acc: 93.07%
Batch 200/1563, Loss: 0.2762, Acc: 93.33%
Batch 300/1563, Loss: 0.1093, Acc: 93.50%
Batch 400/1563, Loss: 0.1875, Acc: 93.49%
Batch 500/1563, Loss: 0.3348, Acc: 93.50%
Batch 600/1563, Loss: 0.2013, Acc: 93.59%
Batch 700/1563, Loss: 0.0605, Acc: 93.41%
Batch 800/1563, Loss: 0.1627, Acc: 93.34%
Batch 900/1563, Loss: 0.3824, Acc: 93.36%
Batch 1000/1563, Loss: 0.2741, Acc: 93.34%
Batch 1100/1563, Loss: 0.2263, Acc: 93.36%
Batch 1200/1563, Loss: 0.3491, Acc: 93.21%
Batch 1300/1563, Loss: 0.0944, Acc: 93.12%
Batch 1400/1563, Loss: 0.2923, Acc: 93.09%
Batch 1500/1563, Loss: 0.1810, Acc: 93.03%
Epoch 32/50:
  Train Loss: 0.1994, Train Acc: 93.01%
  Val Loss: 0.8769, Val Acc: 77.20%
  Time: 502.31s, LR: 0.000086
--------------------------------------------------
Batch 0/1563, Loss: 0.3090, Acc: 93.75%
Batch 100/1563, Loss: 0.2199, Acc: 94.55%
Batch 200/1563, Loss: 0.1459, Acc: 94.45%
Batch 300/1563, Loss: 0.3272, Acc: 94.45%
Batch 400/1563, Loss: 0.2268, Acc: 94.13%
Batch 500/1563, Loss: 0.2452, Acc: 93.98%
Batch 600/1563, Loss: 0.2573, Acc: 93.89%
Batch 700/1563, Loss: 0.0816, Acc: 93.94%
Batch 800/1563, Loss: 0.5001, Acc: 93.86%
Batch 900/1563, Loss: 0.1510, Acc: 93.84%
Batch 1000/1563, Loss: 0.0672, Acc: 93.81%
Batch 1100/1563, Loss: 0.1188, Acc: 93.80%
Batch 1200/1563, Loss: 0.1790, Acc: 93.82%
Batch 1300/1563, Loss: 0.0893, Acc: 93.82%
Batch 1400/1563, Loss: 0.2789, Acc: 93.79%
Batch 1500/1563, Loss: 0.2805, Acc: 93.70%
Epoch 33/50:
  Train Loss: 0.1803, Train Acc: 93.66%
  Val Loss: 0.8882, Val Acc: 77.53%
  Time: 502.60s, LR: 0.000078
  ✓ Nuevo mejor modelo guardado! Val Acc: 77.53%
--------------------------------------------------
Batch 0/1563, Loss: 0.0891, Acc: 96.88%
Batch 100/1563, Loss: 0.1791, Acc: 94.55%
Batch 200/1563, Loss: 0.1821, Acc: 94.56%
Batch 300/1563, Loss: 0.1578, Acc: 94.61%
Batch 400/1563, Loss: 0.2483, Acc: 94.55%
Batch 500/1563, Loss: 0.1551, Acc: 94.62%
Batch 600/1563, Loss: 0.2564, Acc: 94.52%
Batch 700/1563, Loss: 0.0881, Acc: 94.37%
Batch 800/1563, Loss: 0.2832, Acc: 94.31%
Batch 900/1563, Loss: 0.1782, Acc: 94.24%
Batch 1000/1563, Loss: 0.1209, Acc: 94.24%
Batch 1100/1563, Loss: 0.2335, Acc: 94.27%
Batch 1200/1563, Loss: 0.3819, Acc: 94.26%
Batch 1300/1563, Loss: 0.1625, Acc: 94.23%
Batch 1400/1563, Loss: 0.2520, Acc: 94.24%
Batch 1500/1563, Loss: 0.0339, Acc: 94.19%
Epoch 34/50:
  Train Loss: 0.1665, Train Acc: 94.16%
  Val Loss: 0.9181, Val Acc: 77.51%
  Time: 501.90s, LR: 0.000070
--------------------------------------------------
Batch 0/1563, Loss: 0.0927, Acc: 96.88%
Batch 100/1563, Loss: 0.2194, Acc: 95.45%
Batch 200/1563, Loss: 0.1030, Acc: 94.82%
Batch 300/1563, Loss: 0.0681, Acc: 94.85%
Batch 400/1563, Loss: 0.0871, Acc: 94.65%
Batch 500/1563, Loss: 0.0825, Acc: 94.65%
Batch 600/1563, Loss: 0.2139, Acc: 94.69%
Batch 700/1563, Loss: 0.1210, Acc: 94.74%
Batch 800/1563, Loss: 0.1113, Acc: 94.70%
Batch 900/1563, Loss: 0.1305, Acc: 94.75%
Batch 1000/1563, Loss: 0.1586, Acc: 94.76%
Batch 1100/1563, Loss: 0.1624, Acc: 94.74%
Batch 1200/1563, Loss: 0.1705, Acc: 94.77%
Batch 1300/1563, Loss: 0.0934, Acc: 94.76%
Batch 1400/1563, Loss: 0.0280, Acc: 94.79%
Batch 1500/1563, Loss: 0.0325, Acc: 94.75%
Epoch 35/50:
  Train Loss: 0.1498, Train Acc: 94.75%
  Val Loss: 0.9023, Val Acc: 78.01%
  Time: 501.77s, LR: 0.000062
  ✓ Nuevo mejor modelo guardado! Val Acc: 78.01%
--------------------------------------------------
Batch 0/1563, Loss: 0.1617, Acc: 93.75%
Batch 100/1563, Loss: 0.1748, Acc: 95.36%
Batch 200/1563, Loss: 0.1932, Acc: 95.82%
Batch 300/1563, Loss: 0.1013, Acc: 95.89%
Batch 400/1563, Loss: 0.1222, Acc: 95.58%
Batch 500/1563, Loss: 0.0304, Acc: 95.48%
Batch 600/1563, Loss: 0.1590, Acc: 95.53%
Batch 700/1563, Loss: 0.0293, Acc: 95.48%
Batch 800/1563, Loss: 0.0424, Acc: 95.37%
Batch 900/1563, Loss: 0.0294, Acc: 95.39%
Batch 1000/1563, Loss: 0.2001, Acc: 95.43%
Batch 1100/1563, Loss: 0.1822, Acc: 95.37%
Batch 1200/1563, Loss: 0.0684, Acc: 95.33%
Batch 1300/1563, Loss: 0.1610, Acc: 95.31%
Batch 1400/1563, Loss: 0.0912, Acc: 95.28%
Batch 1500/1563, Loss: 0.0606, Acc: 95.29%
Epoch 36/50:
  Train Loss: 0.1374, Train Acc: 95.28%
  Val Loss: 0.9300, Val Acc: 78.00%
  Time: 502.70s, LR: 0.000054
--------------------------------------------------
Batch 0/1563, Loss: 0.0567, Acc: 96.88%
Batch 100/1563, Loss: 0.2465, Acc: 95.70%
Batch 200/1563, Loss: 0.2844, Acc: 95.91%
Batch 300/1563, Loss: 0.1136, Acc: 95.96%
Batch 400/1563, Loss: 0.0198, Acc: 95.98%
Batch 500/1563, Loss: 0.0655, Acc: 95.90%
Batch 600/1563, Loss: 0.0995, Acc: 95.92%
Batch 700/1563, Loss: 0.0515, Acc: 96.02%
Batch 800/1563, Loss: 0.0653, Acc: 95.91%
Batch 900/1563, Loss: 0.1393, Acc: 95.86%
Batch 1000/1563, Loss: 0.3219, Acc: 95.84%
Batch 1100/1563, Loss: 0.0552, Acc: 95.84%
Batch 1200/1563, Loss: 0.0460, Acc: 95.82%
Batch 1300/1563, Loss: 0.0285, Acc: 95.76%
Batch 1400/1563, Loss: 0.1478, Acc: 95.69%
Batch 1500/1563, Loss: 0.1593, Acc: 95.67%
Epoch 37/50:
  Train Loss: 0.1247, Train Acc: 95.67%
  Val Loss: 0.9822, Val Acc: 77.84%
  Time: 502.70s, LR: 0.000047
--------------------------------------------------
Batch 0/1563, Loss: 0.0495, Acc: 100.00%
Batch 100/1563, Loss: 0.0605, Acc: 96.13%
Batch 200/1563, Loss: 0.1696, Acc: 96.42%
Batch 300/1563, Loss: 0.0777, Acc: 96.45%
Batch 400/1563, Loss: 0.2432, Acc: 96.22%
Batch 500/1563, Loss: 0.0158, Acc: 96.27%
Batch 600/1563, Loss: 0.0541, Acc: 96.30%
Batch 700/1563, Loss: 0.1784, Acc: 96.31%
Batch 800/1563, Loss: 0.0773, Acc: 96.37%
Batch 900/1563, Loss: 0.2245, Acc: 96.30%
Batch 1000/1563, Loss: 0.1075, Acc: 96.31%
Batch 1100/1563, Loss: 0.0564, Acc: 96.25%
Batch 1200/1563, Loss: 0.0598, Acc: 96.27%
Batch 1300/1563, Loss: 0.0387, Acc: 96.29%
Batch 1400/1563, Loss: 0.3344, Acc: 96.30%
Batch 1500/1563, Loss: 0.0525, Acc: 96.28%
Epoch 38/50:
  Train Loss: 0.1101, Train Acc: 96.28%
  Val Loss: 0.9580, Val Acc: 78.58%
  Time: 505.45s, LR: 0.000041
  ✓ Nuevo mejor modelo guardado! Val Acc: 78.58%
--------------------------------------------------
Batch 0/1563, Loss: 0.0356, Acc: 100.00%
Batch 100/1563, Loss: 0.1067, Acc: 96.69%
Batch 200/1563, Loss: 0.0352, Acc: 96.46%
Batch 300/1563, Loss: 0.1942, Acc: 96.43%
Batch 400/1563, Loss: 0.0788, Acc: 96.54%
Batch 500/1563, Loss: 0.1810, Acc: 96.69%
Batch 600/1563, Loss: 0.0231, Acc: 96.68%
Batch 700/1563, Loss: 0.1804, Acc: 96.72%
Batch 800/1563, Loss: 0.1717, Acc: 96.65%
Batch 900/1563, Loss: 0.0255, Acc: 96.58%
Batch 1000/1563, Loss: 0.2989, Acc: 96.57%
Batch 1100/1563, Loss: 0.0883, Acc: 96.53%
Batch 1200/1563, Loss: 0.0263, Acc: 96.55%
Batch 1300/1563, Loss: 0.0333, Acc: 96.53%
Batch 1400/1563, Loss: 0.0725, Acc: 96.52%
Batch 1500/1563, Loss: 0.0333, Acc: 96.50%
Epoch 39/50:
  Train Loss: 0.1020, Train Acc: 96.48%
  Val Loss: 0.9927, Val Acc: 78.21%
  Time: 500.89s, LR: 0.000034
--------------------------------------------------
Batch 0/1563, Loss: 0.2053, Acc: 93.75%
Batch 100/1563, Loss: 0.0392, Acc: 97.09%
Batch 200/1563, Loss: 0.1612, Acc: 96.80%
Batch 300/1563, Loss: 0.0125, Acc: 96.80%
Batch 400/1563, Loss: 0.3893, Acc: 96.69%
Batch 500/1563, Loss: 0.0259, Acc: 96.74%
Batch 600/1563, Loss: 0.0247, Acc: 96.81%
Batch 700/1563, Loss: 0.0622, Acc: 96.78%
Batch 800/1563, Loss: 0.2328, Acc: 96.77%
Batch 900/1563, Loss: 0.0754, Acc: 96.83%
Batch 1000/1563, Loss: 0.3003, Acc: 96.82%
Batch 1100/1563, Loss: 0.0664, Acc: 96.78%
Batch 1200/1563, Loss: 0.0853, Acc: 96.74%
Batch 1300/1563, Loss: 0.2051, Acc: 96.74%
Batch 1400/1563, Loss: 0.0271, Acc: 96.78%
Batch 1500/1563, Loss: 0.0753, Acc: 96.80%
Epoch 40/50:
  Train Loss: 0.0946, Train Acc: 96.81%
  Val Loss: 1.0116, Val Acc: 78.79%
  Time: 500.80s, LR: 0.000029
  ✓ Nuevo mejor modelo guardado! Val Acc: 78.79%
--------------------------------------------------
Batch 0/1563, Loss: 0.2876, Acc: 93.75%
Batch 100/1563, Loss: 0.0736, Acc: 96.66%
Batch 200/1563, Loss: 0.0233, Acc: 96.94%
Batch 300/1563, Loss: 0.0244, Acc: 97.07%
Batch 400/1563, Loss: 0.2008, Acc: 97.02%
Batch 500/1563, Loss: 0.1768, Acc: 97.03%
Batch 600/1563, Loss: 0.1038, Acc: 96.93%
Batch 700/1563, Loss: 0.0434, Acc: 97.04%
Batch 800/1563, Loss: 0.1568, Acc: 97.01%
Batch 900/1563, Loss: 0.1849, Acc: 97.00%
Batch 1000/1563, Loss: 0.0204, Acc: 97.01%
Batch 1100/1563, Loss: 0.0402, Acc: 97.00%
Batch 1200/1563, Loss: 0.1199, Acc: 97.04%
Batch 1300/1563, Loss: 0.0955, Acc: 97.05%
Batch 1400/1563, Loss: 0.0268, Acc: 97.07%
Batch 1500/1563, Loss: 0.0075, Acc: 97.07%
Epoch 41/50:
  Train Loss: 0.0867, Train Acc: 97.06%
  Val Loss: 1.0245, Val Acc: 78.52%
  Time: 501.32s, LR: 0.000023
--------------------------------------------------
Batch 0/1563, Loss: 0.0321, Acc: 100.00%
Batch 100/1563, Loss: 0.0446, Acc: 97.59%
Batch 200/1563, Loss: 0.0107, Acc: 97.43%
Batch 300/1563, Loss: 0.0455, Acc: 97.29%
Batch 400/1563, Loss: 0.0969, Acc: 97.40%
Batch 500/1563, Loss: 0.1607, Acc: 97.39%
Batch 600/1563, Loss: 0.0552, Acc: 97.38%
Batch 700/1563, Loss: 0.0236, Acc: 97.35%
Batch 800/1563, Loss: 0.2309, Acc: 97.33%
Batch 900/1563, Loss: 0.0294, Acc: 97.34%
Batch 1000/1563, Loss: 0.0784, Acc: 97.32%
Batch 1100/1563, Loss: 0.0367, Acc: 97.30%
Batch 1200/1563, Loss: 0.0554, Acc: 97.33%
Batch 1300/1563, Loss: 0.1859, Acc: 97.33%
Batch 1400/1563, Loss: 0.0093, Acc: 97.34%
Batch 1500/1563, Loss: 0.0116, Acc: 97.35%
Epoch 42/50:
  Train Loss: 0.0800, Train Acc: 97.35%
  Val Loss: 1.0376, Val Acc: 78.85%
  Time: 502.17s, LR: 0.000019
  ✓ Nuevo mejor modelo guardado! Val Acc: 78.85%
--------------------------------------------------
Batch 0/1563, Loss: 0.0040, Acc: 100.00%
Batch 100/1563, Loss: 0.1900, Acc: 97.34%
Batch 200/1563, Loss: 0.2153, Acc: 97.40%
Batch 300/1563, Loss: 0.0298, Acc: 97.41%
Batch 400/1563, Loss: 0.0091, Acc: 97.31%
Batch 500/1563, Loss: 0.1548, Acc: 97.46%
Batch 600/1563, Loss: 0.0134, Acc: 97.44%
Batch 700/1563, Loss: 0.0175, Acc: 97.45%
Batch 800/1563, Loss: 0.1441, Acc: 97.49%
Batch 900/1563, Loss: 0.0257, Acc: 97.56%
Batch 1000/1563, Loss: 0.2161, Acc: 97.56%
Batch 1100/1563, Loss: 0.0509, Acc: 97.58%
Batch 1200/1563, Loss: 0.0245, Acc: 97.60%
Batch 1300/1563, Loss: 0.1489, Acc: 97.59%
Batch 1400/1563, Loss: 0.0275, Acc: 97.56%
Batch 1500/1563, Loss: 0.0663, Acc: 97.55%
Epoch 43/50:
  Train Loss: 0.0768, Train Acc: 97.55%
  Val Loss: 1.0182, Val Acc: 78.84%
  Time: 503.46s, LR: 0.000014
--------------------------------------------------
Batch 0/1563, Loss: 0.1030, Acc: 93.75%
Batch 100/1563, Loss: 0.0202, Acc: 97.74%
Batch 200/1563, Loss: 0.0783, Acc: 97.73%
Batch 300/1563, Loss: 0.0805, Acc: 97.80%
Batch 400/1563, Loss: 0.0629, Acc: 97.79%
Batch 500/1563, Loss: 0.0902, Acc: 97.86%
Batch 600/1563, Loss: 0.0303, Acc: 97.86%
Batch 700/1563, Loss: 0.0581, Acc: 97.87%
Batch 800/1563, Loss: 0.0071, Acc: 97.85%
Batch 900/1563, Loss: 0.0805, Acc: 97.79%
Batch 1000/1563, Loss: 0.0698, Acc: 97.78%
Batch 1100/1563, Loss: 0.0336, Acc: 97.77%
Batch 1200/1563, Loss: 0.0175, Acc: 97.77%
Batch 1300/1563, Loss: 0.0620, Acc: 97.76%
Batch 1400/1563, Loss: 0.0211, Acc: 97.75%
Batch 1500/1563, Loss: 0.2072, Acc: 97.75%
Epoch 44/50:
  Train Loss: 0.0696, Train Acc: 97.73%
  Val Loss: 1.0354, Val Acc: 79.05%
  Time: 503.62s, LR: 0.000011
  ✓ Nuevo mejor modelo guardado! Val Acc: 79.05%
--------------------------------------------------
Batch 0/1563, Loss: 0.0727, Acc: 100.00%
Batch 100/1563, Loss: 0.0140, Acc: 97.68%
Batch 200/1563, Loss: 0.2224, Acc: 97.64%
Batch 300/1563, Loss: 0.0130, Acc: 97.64%
Batch 400/1563, Loss: 0.0151, Acc: 97.75%
Batch 500/1563, Loss: 0.1353, Acc: 97.79%
Batch 600/1563, Loss: 0.0804, Acc: 97.72%
Batch 700/1563, Loss: 0.0056, Acc: 97.74%
Batch 800/1563, Loss: 0.0149, Acc: 97.75%
Batch 900/1563, Loss: 0.0664, Acc: 97.74%
Batch 1000/1563, Loss: 0.0382, Acc: 97.75%
Batch 1100/1563, Loss: 0.0272, Acc: 97.76%
Batch 1200/1563, Loss: 0.0421, Acc: 97.78%
Batch 1300/1563, Loss: 0.0566, Acc: 97.75%
Batch 1400/1563, Loss: 0.0781, Acc: 97.74%
Batch 1500/1563, Loss: 0.0980, Acc: 97.74%
Epoch 45/50:
  Train Loss: 0.0684, Train Acc: 97.76%
  Val Loss: 1.0515, Val Acc: 79.01%
  Time: 508.65s, LR: 0.000007
--------------------------------------------------
Batch 0/1563, Loss: 0.0223, Acc: 100.00%
Batch 100/1563, Loss: 0.1016, Acc: 97.93%
Batch 200/1563, Loss: 0.0603, Acc: 97.84%
Batch 300/1563, Loss: 0.2421, Acc: 97.94%
Batch 400/1563, Loss: 0.0401, Acc: 97.84%
Batch 500/1563, Loss: 0.0208, Acc: 97.90%
Batch 600/1563, Loss: 0.0479, Acc: 97.91%
Batch 700/1563, Loss: 0.0201, Acc: 97.90%
Batch 800/1563, Loss: 0.0501, Acc: 97.98%
Batch 900/1563, Loss: 0.0083, Acc: 97.96%
Batch 1000/1563, Loss: 0.0512, Acc: 97.95%
Batch 1100/1563, Loss: 0.0694, Acc: 97.96%
Batch 1200/1563, Loss: 0.0220, Acc: 97.92%
Batch 1300/1563, Loss: 0.0573, Acc: 97.87%
Batch 1400/1563, Loss: 0.0669, Acc: 97.89%
Batch 1500/1563, Loss: 0.0849, Acc: 97.87%
Epoch 46/50:
  Train Loss: 0.0639, Train Acc: 97.89%
  Val Loss: 1.0439, Val Acc: 79.37%
  Time: 505.93s, LR: 0.000005
  ✓ Nuevo mejor modelo guardado! Val Acc: 79.37%
--------------------------------------------------
Batch 0/1563, Loss: 0.0514, Acc: 100.00%
Batch 100/1563, Loss: 0.2110, Acc: 97.56%
Batch 200/1563, Loss: 0.1134, Acc: 97.90%
Batch 300/1563, Loss: 0.0122, Acc: 97.85%
Batch 400/1563, Loss: 0.1604, Acc: 97.84%
Batch 500/1563, Loss: 0.0847, Acc: 97.85%
Batch 600/1563, Loss: 0.0588, Acc: 97.84%
Batch 700/1563, Loss: 0.0081, Acc: 97.84%
Batch 800/1563, Loss: 0.0370, Acc: 97.83%
Batch 900/1563, Loss: 0.0514, Acc: 97.84%
Batch 1000/1563, Loss: 0.0982, Acc: 97.82%
Batch 1100/1563, Loss: 0.0968, Acc: 97.87%
Batch 1200/1563, Loss: 0.0021, Acc: 97.85%
Batch 1300/1563, Loss: 0.0573, Acc: 97.88%
Batch 1400/1563, Loss: 0.1755, Acc: 97.91%
Batch 1500/1563, Loss: 0.0524, Acc: 97.92%
Epoch 47/50:
  Train Loss: 0.0628, Train Acc: 97.92%
  Val Loss: 1.0492, Val Acc: 79.23%
  Time: 506.32s, LR: 0.000003
--------------------------------------------------
Batch 0/1563, Loss: 0.0099, Acc: 100.00%
Batch 100/1563, Loss: 0.0103, Acc: 98.36%
Batch 200/1563, Loss: 0.0117, Acc: 98.24%
Batch 300/1563, Loss: 0.0273, Acc: 98.06%
Batch 400/1563, Loss: 0.0296, Acc: 98.00%
Batch 500/1563, Loss: 0.0943, Acc: 98.06%
Batch 600/1563, Loss: 0.1000, Acc: 98.06%
Batch 700/1563, Loss: 0.1941, Acc: 98.06%
Batch 800/1563, Loss: 0.0257, Acc: 98.10%
Batch 900/1563, Loss: 0.0127, Acc: 98.05%
Batch 1000/1563, Loss: 0.0238, Acc: 98.01%
Batch 1100/1563, Loss: 0.0080, Acc: 98.03%
Batch 1200/1563, Loss: 0.0798, Acc: 98.08%
Batch 1300/1563, Loss: 0.0341, Acc: 98.10%
Batch 1400/1563, Loss: 0.0072, Acc: 98.10%
Batch 1500/1563, Loss: 0.1422, Acc: 98.08%
Epoch 48/50:
  Train Loss: 0.0588, Train Acc: 98.08%
  Val Loss: 1.0520, Val Acc: 79.17%
  Time: 506.84s, LR: 0.000001
--------------------------------------------------
Batch 0/1563, Loss: 0.0546, Acc: 96.88%
Batch 100/1563, Loss: 0.0476, Acc: 98.17%
Batch 200/1563, Loss: 0.1629, Acc: 98.17%
Batch 300/1563, Loss: 0.0598, Acc: 98.29%
Batch 400/1563, Loss: 0.0261, Acc: 98.24%
Batch 500/1563, Loss: 0.0201, Acc: 98.29%
Batch 600/1563, Loss: 0.0037, Acc: 98.26%
Batch 700/1563, Loss: 0.2260, Acc: 98.23%
Batch 800/1563, Loss: 0.0144, Acc: 98.25%
Batch 900/1563, Loss: 0.1510, Acc: 98.20%
Batch 1000/1563, Loss: 0.2548, Acc: 98.16%
Batch 1100/1563, Loss: 0.0363, Acc: 98.12%
Batch 1200/1563, Loss: 0.1560, Acc: 98.07%
Batch 1300/1563, Loss: 0.0103, Acc: 98.06%
Batch 1400/1563, Loss: 0.0331, Acc: 98.10%
Batch 1500/1563, Loss: 0.1407, Acc: 98.12%
Epoch 49/50:
  Train Loss: 0.0587, Train Acc: 98.13%
  Val Loss: 1.0492, Val Acc: 79.22%
  Time: 507.76s, LR: 0.000000
--------------------------------------------------
Batch 0/1563, Loss: 0.0451, Acc: 96.88%
Batch 100/1563, Loss: 0.0440, Acc: 98.21%
Batch 200/1563, Loss: 0.0322, Acc: 98.06%
Batch 300/1563, Loss: 0.0089, Acc: 98.24%
Batch 400/1563, Loss: 0.1194, Acc: 98.25%
Batch 500/1563, Loss: 0.0213, Acc: 98.19%
Batch 600/1563, Loss: 0.0227, Acc: 98.20%
Batch 700/1563, Loss: 0.0102, Acc: 98.14%
Batch 800/1563, Loss: 0.0484, Acc: 98.17%
Batch 900/1563, Loss: 0.0696, Acc: 98.12%
Batch 1000/1563, Loss: 0.1053, Acc: 98.11%
Batch 1100/1563, Loss: 0.0331, Acc: 98.11%
Batch 1200/1563, Loss: 0.0037, Acc: 98.12%
Batch 1300/1563, Loss: 0.0549, Acc: 98.12%
Batch 1400/1563, Loss: 0.0252, Acc: 98.10%
Batch 1500/1563, Loss: 0.1263, Acc: 98.10%
Epoch 50/50:
  Train Loss: 0.0586, Train Acc: 98.09%
  Val Loss: 1.0470, Val Acc: 79.32%
  Time: 503.07s, LR: 0.000000
--------------------------------------------------
Entrenamiento completado. Mejor accuracy de validación: 79.37%