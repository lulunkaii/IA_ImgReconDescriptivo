{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1350,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 0.9562236070632935,
      "learning_rate": 4.962962962962963e-05,
      "loss": 4.5113,
      "step": 10
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 0.8585706353187561,
      "learning_rate": 4.925925925925926e-05,
      "loss": 4.5114,
      "step": 20
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.8449008464813232,
      "learning_rate": 4.888888888888889e-05,
      "loss": 4.5142,
      "step": 30
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 0.9243622422218323,
      "learning_rate": 4.851851851851852e-05,
      "loss": 4.496,
      "step": 40
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 1.010651707649231,
      "learning_rate": 4.814814814814815e-05,
      "loss": 4.5028,
      "step": 50
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.9871514439582825,
      "learning_rate": 4.7777777777777784e-05,
      "loss": 4.5112,
      "step": 60
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 0.8844282627105713,
      "learning_rate": 4.740740740740741e-05,
      "loss": 4.4963,
      "step": 70
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 0.7638373374938965,
      "learning_rate": 4.703703703703704e-05,
      "loss": 4.5273,
      "step": 80
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.7102782726287842,
      "learning_rate": 4.666666666666667e-05,
      "loss": 4.5088,
      "step": 90
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 0.6424890756607056,
      "learning_rate": 4.62962962962963e-05,
      "loss": 4.5062,
      "step": 100
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 0.666974663734436,
      "learning_rate": 4.592592592592593e-05,
      "loss": 4.5123,
      "step": 110
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.5801838040351868,
      "learning_rate": 4.555555555555556e-05,
      "loss": 4.5076,
      "step": 120
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 0.5534734129905701,
      "learning_rate": 4.518518518518519e-05,
      "loss": 4.5017,
      "step": 130
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 0.6143471002578735,
      "learning_rate": 4.481481481481482e-05,
      "loss": 4.5034,
      "step": 140
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.585290253162384,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 4.5061,
      "step": 150
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 0.559522807598114,
      "learning_rate": 4.4074074074074076e-05,
      "loss": 4.5117,
      "step": 160
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 0.5356360673904419,
      "learning_rate": 4.3703703703703705e-05,
      "loss": 4.5191,
      "step": 170
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.5365258455276489,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 4.5008,
      "step": 180
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 0.5546309947967529,
      "learning_rate": 4.296296296296296e-05,
      "loss": 4.5036,
      "step": 190
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.5558744668960571,
      "learning_rate": 4.259259259259259e-05,
      "loss": 4.5051,
      "step": 200
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.5123218297958374,
      "learning_rate": 4.222222222222222e-05,
      "loss": 4.5105,
      "step": 210
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 0.5596892237663269,
      "learning_rate": 4.185185185185185e-05,
      "loss": 4.511,
      "step": 220
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 0.471387654542923,
      "learning_rate": 4.148148148148148e-05,
      "loss": 4.5084,
      "step": 230
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.5531173348426819,
      "learning_rate": 4.111111111111111e-05,
      "loss": 4.5038,
      "step": 240
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 0.5033326148986816,
      "learning_rate": 4.074074074074074e-05,
      "loss": 4.5072,
      "step": 250
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 0.5050350427627563,
      "learning_rate": 4.0370370370370374e-05,
      "loss": 4.5103,
      "step": 260
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4959891736507416,
      "learning_rate": 4e-05,
      "loss": 4.5084,
      "step": 270
    },
    {
      "epoch": 1.0,
      "eval_loss": 4.500182151794434,
      "eval_runtime": 30.5313,
      "eval_samples_per_second": 35.373,
      "eval_steps_per_second": 2.227,
      "step": 270
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 0.4942033290863037,
      "learning_rate": 3.962962962962963e-05,
      "loss": 4.5015,
      "step": 280
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 0.5240698456764221,
      "learning_rate": 3.925925925925926e-05,
      "loss": 4.5026,
      "step": 290
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.5886160135269165,
      "learning_rate": 3.888888888888889e-05,
      "loss": 4.5012,
      "step": 300
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 0.5719890594482422,
      "learning_rate": 3.851851851851852e-05,
      "loss": 4.5025,
      "step": 310
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 0.4927033483982086,
      "learning_rate": 3.814814814814815e-05,
      "loss": 4.4988,
      "step": 320
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.4949144721031189,
      "learning_rate": 3.777777777777778e-05,
      "loss": 4.5028,
      "step": 330
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 0.5179809927940369,
      "learning_rate": 3.740740740740741e-05,
      "loss": 4.5016,
      "step": 340
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 0.5088649392127991,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 4.505,
      "step": 350
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.52507483959198,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 4.5061,
      "step": 360
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 0.4920429587364197,
      "learning_rate": 3.62962962962963e-05,
      "loss": 4.5057,
      "step": 370
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 0.4548654556274414,
      "learning_rate": 3.592592592592593e-05,
      "loss": 4.503,
      "step": 380
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.5258389115333557,
      "learning_rate": 3.555555555555556e-05,
      "loss": 4.5033,
      "step": 390
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 0.5544663667678833,
      "learning_rate": 3.518518518518519e-05,
      "loss": 4.5017,
      "step": 400
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 0.5601581335067749,
      "learning_rate": 3.481481481481482e-05,
      "loss": 4.5041,
      "step": 410
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.5500606894493103,
      "learning_rate": 3.444444444444445e-05,
      "loss": 4.5028,
      "step": 420
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 0.48980513215065,
      "learning_rate": 3.4074074074074077e-05,
      "loss": 4.5047,
      "step": 430
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 0.4679275155067444,
      "learning_rate": 3.3703703703703706e-05,
      "loss": 4.5051,
      "step": 440
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.5507869124412537,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 4.5063,
      "step": 450
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 0.5702100396156311,
      "learning_rate": 3.2962962962962964e-05,
      "loss": 4.5032,
      "step": 460
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 0.5743423104286194,
      "learning_rate": 3.25925925925926e-05,
      "loss": 4.5067,
      "step": 470
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.49947434663772583,
      "learning_rate": 3.222222222222223e-05,
      "loss": 4.5094,
      "step": 480
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 0.4444058835506439,
      "learning_rate": 3.185185185185185e-05,
      "loss": 4.5028,
      "step": 490
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 0.49021345376968384,
      "learning_rate": 3.148148148148148e-05,
      "loss": 4.4992,
      "step": 500
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.4729512333869934,
      "learning_rate": 3.111111111111111e-05,
      "loss": 4.5089,
      "step": 510
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 0.4823577105998993,
      "learning_rate": 3.074074074074074e-05,
      "loss": 4.4997,
      "step": 520
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 0.45178160071372986,
      "learning_rate": 3.037037037037037e-05,
      "loss": 4.5083,
      "step": 530
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.4370918273925781,
      "learning_rate": 3e-05,
      "loss": 4.5099,
      "step": 540
    },
    {
      "epoch": 2.0,
      "eval_loss": 4.5000481605529785,
      "eval_runtime": 23.6062,
      "eval_samples_per_second": 45.751,
      "eval_steps_per_second": 2.881,
      "step": 540
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 0.519666850566864,
      "learning_rate": 2.962962962962963e-05,
      "loss": 4.5,
      "step": 550
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 0.45378217101097107,
      "learning_rate": 2.925925925925926e-05,
      "loss": 4.5004,
      "step": 560
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.4932536482810974,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 4.4974,
      "step": 570
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 0.47943997383117676,
      "learning_rate": 2.851851851851852e-05,
      "loss": 4.5035,
      "step": 580
    },
    {
      "epoch": 2.185185185185185,
      "grad_norm": 0.48337382078170776,
      "learning_rate": 2.814814814814815e-05,
      "loss": 4.5034,
      "step": 590
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.4751507639884949,
      "learning_rate": 2.777777777777778e-05,
      "loss": 4.4993,
      "step": 600
    },
    {
      "epoch": 2.259259259259259,
      "grad_norm": 0.6007166504859924,
      "learning_rate": 2.7407407407407408e-05,
      "loss": 4.5018,
      "step": 610
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 0.5394887924194336,
      "learning_rate": 2.7037037037037037e-05,
      "loss": 4.5012,
      "step": 620
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.47969141602516174,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 4.5039,
      "step": 630
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 0.5026612281799316,
      "learning_rate": 2.6296296296296296e-05,
      "loss": 4.5021,
      "step": 640
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 0.47685104608535767,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 4.4997,
      "step": 650
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.4973064064979553,
      "learning_rate": 2.5555555555555554e-05,
      "loss": 4.5052,
      "step": 660
    },
    {
      "epoch": 2.4814814814814814,
      "grad_norm": 0.5028828382492065,
      "learning_rate": 2.5185185185185183e-05,
      "loss": 4.5032,
      "step": 670
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 0.4781235456466675,
      "learning_rate": 2.4814814814814816e-05,
      "loss": 4.5016,
      "step": 680
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.5708016157150269,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 4.4984,
      "step": 690
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 0.49113792181015015,
      "learning_rate": 2.4074074074074074e-05,
      "loss": 4.5016,
      "step": 700
    },
    {
      "epoch": 2.6296296296296298,
      "grad_norm": 0.47227564454078674,
      "learning_rate": 2.3703703703703707e-05,
      "loss": 4.5068,
      "step": 710
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.47834232449531555,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 4.5032,
      "step": 720
    },
    {
      "epoch": 2.7037037037037037,
      "grad_norm": 0.48386624455451965,
      "learning_rate": 2.2962962962962965e-05,
      "loss": 4.5021,
      "step": 730
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 0.5106997489929199,
      "learning_rate": 2.2592592592592594e-05,
      "loss": 4.5057,
      "step": 740
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.5201141834259033,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 4.5036,
      "step": 750
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 0.5460297465324402,
      "learning_rate": 2.1851851851851852e-05,
      "loss": 4.5032,
      "step": 760
    },
    {
      "epoch": 2.851851851851852,
      "grad_norm": 0.47917982935905457,
      "learning_rate": 2.148148148148148e-05,
      "loss": 4.5053,
      "step": 770
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.4657255709171295,
      "learning_rate": 2.111111111111111e-05,
      "loss": 4.5038,
      "step": 780
    },
    {
      "epoch": 2.925925925925926,
      "grad_norm": 0.4575391113758087,
      "learning_rate": 2.074074074074074e-05,
      "loss": 4.5041,
      "step": 790
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 0.5198403596878052,
      "learning_rate": 2.037037037037037e-05,
      "loss": 4.5046,
      "step": 800
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.41979867219924927,
      "learning_rate": 2e-05,
      "loss": 4.5046,
      "step": 810
    },
    {
      "epoch": 3.0,
      "eval_loss": 4.499922275543213,
      "eval_runtime": 23.7816,
      "eval_samples_per_second": 45.413,
      "eval_steps_per_second": 2.859,
      "step": 810
    },
    {
      "epoch": 3.037037037037037,
      "grad_norm": 0.48838576674461365,
      "learning_rate": 1.962962962962963e-05,
      "loss": 4.5001,
      "step": 820
    },
    {
      "epoch": 3.074074074074074,
      "grad_norm": 0.5035366415977478,
      "learning_rate": 1.925925925925926e-05,
      "loss": 4.5023,
      "step": 830
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.5002865791320801,
      "learning_rate": 1.888888888888889e-05,
      "loss": 4.4998,
      "step": 840
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 0.4522261321544647,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 4.5037,
      "step": 850
    },
    {
      "epoch": 3.185185185185185,
      "grad_norm": 0.5097435116767883,
      "learning_rate": 1.814814814814815e-05,
      "loss": 4.5004,
      "step": 860
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.4540969729423523,
      "learning_rate": 1.777777777777778e-05,
      "loss": 4.4983,
      "step": 870
    },
    {
      "epoch": 3.259259259259259,
      "grad_norm": 0.5493395328521729,
      "learning_rate": 1.740740740740741e-05,
      "loss": 4.5008,
      "step": 880
    },
    {
      "epoch": 3.2962962962962963,
      "grad_norm": 0.48864126205444336,
      "learning_rate": 1.7037037037037038e-05,
      "loss": 4.5016,
      "step": 890
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.5478765964508057,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 4.5013,
      "step": 900
    },
    {
      "epoch": 3.3703703703703702,
      "grad_norm": 0.4998348653316498,
      "learning_rate": 1.62962962962963e-05,
      "loss": 4.5042,
      "step": 910
    },
    {
      "epoch": 3.4074074074074074,
      "grad_norm": 0.47095391154289246,
      "learning_rate": 1.5925925925925926e-05,
      "loss": 4.501,
      "step": 920
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.42510369420051575,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 4.5014,
      "step": 930
    },
    {
      "epoch": 3.4814814814814814,
      "grad_norm": 0.5008190274238586,
      "learning_rate": 1.5185185185185186e-05,
      "loss": 4.502,
      "step": 940
    },
    {
      "epoch": 3.5185185185185186,
      "grad_norm": 0.42206618189811707,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 4.5012,
      "step": 950
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.48039212822914124,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 4.5016,
      "step": 960
    },
    {
      "epoch": 3.5925925925925926,
      "grad_norm": 0.4362257421016693,
      "learning_rate": 1.4074074074074075e-05,
      "loss": 4.5016,
      "step": 970
    },
    {
      "epoch": 3.6296296296296298,
      "grad_norm": 0.49281683564186096,
      "learning_rate": 1.3703703703703704e-05,
      "loss": 4.5007,
      "step": 980
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.4713204503059387,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 4.5037,
      "step": 990
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 0.5311527848243713,
      "learning_rate": 1.2962962962962962e-05,
      "loss": 4.5016,
      "step": 1000
    },
    {
      "epoch": 3.7407407407407405,
      "grad_norm": 0.4518832862377167,
      "learning_rate": 1.2592592592592592e-05,
      "loss": 4.5024,
      "step": 1010
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.4698091149330139,
      "learning_rate": 1.2222222222222222e-05,
      "loss": 4.5009,
      "step": 1020
    },
    {
      "epoch": 3.814814814814815,
      "grad_norm": 0.47274142503738403,
      "learning_rate": 1.1851851851851853e-05,
      "loss": 4.5018,
      "step": 1030
    },
    {
      "epoch": 3.851851851851852,
      "grad_norm": 0.5004698634147644,
      "learning_rate": 1.1481481481481482e-05,
      "loss": 4.5035,
      "step": 1040
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.4769676923751831,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 4.5014,
      "step": 1050
    },
    {
      "epoch": 3.925925925925926,
      "grad_norm": 0.48006144165992737,
      "learning_rate": 1.074074074074074e-05,
      "loss": 4.5026,
      "step": 1060
    },
    {
      "epoch": 3.962962962962963,
      "grad_norm": 0.5065310001373291,
      "learning_rate": 1.037037037037037e-05,
      "loss": 4.5019,
      "step": 1070
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.467630535364151,
      "learning_rate": 1e-05,
      "loss": 4.5034,
      "step": 1080
    },
    {
      "epoch": 4.0,
      "eval_loss": 4.49989128112793,
      "eval_runtime": 23.075,
      "eval_samples_per_second": 46.804,
      "eval_steps_per_second": 2.947,
      "step": 1080
    },
    {
      "epoch": 4.037037037037037,
      "grad_norm": 0.46172159910202026,
      "learning_rate": 9.62962962962963e-06,
      "loss": 4.4997,
      "step": 1090
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 0.5025491714477539,
      "learning_rate": 9.259259259259259e-06,
      "loss": 4.5001,
      "step": 1100
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 0.43596482276916504,
      "learning_rate": 8.88888888888889e-06,
      "loss": 4.5,
      "step": 1110
    },
    {
      "epoch": 4.148148148148148,
      "grad_norm": 0.49146246910095215,
      "learning_rate": 8.518518518518519e-06,
      "loss": 4.4989,
      "step": 1120
    },
    {
      "epoch": 4.185185185185185,
      "grad_norm": 0.44080856442451477,
      "learning_rate": 8.14814814814815e-06,
      "loss": 4.4997,
      "step": 1130
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 0.44910717010498047,
      "learning_rate": 7.777777777777777e-06,
      "loss": 4.4999,
      "step": 1140
    },
    {
      "epoch": 4.2592592592592595,
      "grad_norm": 0.4970763623714447,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 4.4997,
      "step": 1150
    },
    {
      "epoch": 4.296296296296296,
      "grad_norm": 0.5059677958488464,
      "learning_rate": 7.0370370370370375e-06,
      "loss": 4.5018,
      "step": 1160
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.42066654562950134,
      "learning_rate": 6.666666666666667e-06,
      "loss": 4.4963,
      "step": 1170
    },
    {
      "epoch": 4.37037037037037,
      "grad_norm": 0.5345383286476135,
      "learning_rate": 6.296296296296296e-06,
      "loss": 4.5024,
      "step": 1180
    },
    {
      "epoch": 4.407407407407407,
      "grad_norm": 0.42226412892341614,
      "learning_rate": 5.925925925925927e-06,
      "loss": 4.5006,
      "step": 1190
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 0.4509904682636261,
      "learning_rate": 5.555555555555556e-06,
      "loss": 4.503,
      "step": 1200
    },
    {
      "epoch": 4.481481481481482,
      "grad_norm": 0.490961492061615,
      "learning_rate": 5.185185185185185e-06,
      "loss": 4.5025,
      "step": 1210
    },
    {
      "epoch": 4.518518518518518,
      "grad_norm": 0.494236558675766,
      "learning_rate": 4.814814814814815e-06,
      "loss": 4.5006,
      "step": 1220
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 0.49636906385421753,
      "learning_rate": 4.444444444444445e-06,
      "loss": 4.4989,
      "step": 1230
    },
    {
      "epoch": 4.592592592592593,
      "grad_norm": 0.48441359400749207,
      "learning_rate": 4.074074074074075e-06,
      "loss": 4.4998,
      "step": 1240
    },
    {
      "epoch": 4.62962962962963,
      "grad_norm": 0.4934420585632324,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 4.5016,
      "step": 1250
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.41628003120422363,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 4.5015,
      "step": 1260
    },
    {
      "epoch": 4.703703703703704,
      "grad_norm": 0.5313355922698975,
      "learning_rate": 2.9629629629629633e-06,
      "loss": 4.5014,
      "step": 1270
    },
    {
      "epoch": 4.7407407407407405,
      "grad_norm": 0.470122754573822,
      "learning_rate": 2.5925925925925925e-06,
      "loss": 4.5006,
      "step": 1280
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 0.45576581358909607,
      "learning_rate": 2.2222222222222225e-06,
      "loss": 4.4999,
      "step": 1290
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 0.4831589460372925,
      "learning_rate": 1.8518518518518519e-06,
      "loss": 4.5031,
      "step": 1300
    },
    {
      "epoch": 4.851851851851852,
      "grad_norm": 0.4786911606788635,
      "learning_rate": 1.4814814814814817e-06,
      "loss": 4.4998,
      "step": 1310
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 0.48802798986434937,
      "learning_rate": 1.1111111111111112e-06,
      "loss": 4.5034,
      "step": 1320
    },
    {
      "epoch": 4.925925925925926,
      "grad_norm": 0.4675961434841156,
      "learning_rate": 7.407407407407408e-07,
      "loss": 4.5017,
      "step": 1330
    },
    {
      "epoch": 4.962962962962963,
      "grad_norm": 0.4319494664669037,
      "learning_rate": 3.703703703703704e-07,
      "loss": 4.5013,
      "step": 1340
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.4640175402164459,
      "learning_rate": 0.0,
      "loss": 4.5012,
      "step": 1350
    },
    {
      "epoch": 5.0,
      "eval_loss": 4.499917507171631,
      "eval_runtime": 23.1079,
      "eval_samples_per_second": 46.737,
      "eval_steps_per_second": 2.943,
      "step": 1350
    }
  ],
  "logging_steps": 10,
  "max_steps": 1350,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 1.6751471500689408e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
